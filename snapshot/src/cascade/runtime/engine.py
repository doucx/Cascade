import time
import inspect
import asyncio
from typing import Any, Dict, Optional, Generator, Callable, List
from uuid import uuid4
from contextlib import ExitStack

from cascade.graph.build import build_graph
from cascade.graph.model import Node, Graph
from cascade.spec.resource import ResourceDefinition, Inject
from cascade.runtime.bus import MessageBus
from cascade.runtime.events import (
    RunStarted,
    RunFinished,
    TaskExecutionStarted,
    TaskExecutionFinished,
    TaskSkipped,
    TaskRetrying,
    ResourceAcquired,
    ResourceReleased,
)
from cascade.runtime.protocols import Solver, Executor
from cascade.adapters.solvers.native import NativeSolver
from cascade.adapters.executors.local import LocalExecutor
from cascade.runtime.resource_manager import ResourceManager
from cascade.spec.task import LazyResult, MappedLazyResult


class Engine:
    """
    Orchestrates the entire workflow execution.
    """

    def __init__(
        self,
        solver: Optional[Solver] = None,
        executor: Optional[Executor] = None,
        bus: Optional[MessageBus] = None,
        system_resources: Optional[Dict[str, Any]] = None,
    ):
        self.solver = solver or NativeSolver()
        self.executor = executor or LocalExecutor()
        self.bus = bus or MessageBus()
        self.resource_manager = ResourceManager(capacity=system_resources)
        self._resource_providers: Dict[str, Callable] = {}

    def register(self, resource_def: ResourceDefinition):
        """Registers a resource provider function with the engine."""
        self._resource_providers[resource_def.name] = resource_def.func

    def get_resource_provider(self, name: str) -> Callable:
        return self._resource_providers[name]

    def _inject_params(
        self, plan: list[Node], user_params: Dict[str, Any], results: Dict[str, Any]
    ):
        for node in plan:
            if node.node_type == "param":
                param_spec = node.param_spec
                if node.name in user_params:
                    results[node.id] = user_params[node.name]
                elif param_spec.default is not None:
                    results[node.id] = param_spec.default
                else:
                    raise ValueError(
                        f"Required parameter '{node.name}' was not provided."
                    )

    def _should_skip(
        self,
        node: Node,
        graph: Graph,
        results: Dict[str, Any],
        skipped_node_ids: set[str],
    ) -> Optional[str]:
        """
        Determines if a node should be skipped.
        Returns the reason string if yes, None otherwise.
        """
        incoming_edges = [edge for edge in graph.edges if edge.target.id == node.id]

        # 1. Cascade Skip: If any upstream dependency was skipped
        for edge in incoming_edges:
            if edge.source.id in skipped_node_ids:
                return "UpstreamSkipped"

        # 2. Condition Check: If this node has a condition and it evaluated to False
        for edge in incoming_edges:
            if edge.arg_name == "_condition":
                condition_result = results.get(edge.source.id)
                if not condition_result:
                    return "ConditionFalse"

        return None

    def override_resource_provider(self, name: str, new_provider: Any):
        # Unwrap ResourceDefinition if provided
        if isinstance(new_provider, ResourceDefinition):
            new_provider = new_provider.func
        self._resource_providers[name] = new_provider

    async def run(self, target: Any, params: Optional[Dict[str, Any]] = None) -> Any:
        run_id = str(uuid4())
        start_time = time.time()

        # Handle MappedLazyResult or LazyResult
        target_name = getattr(target, "name", "unknown")
        if hasattr(target, "task"):
            target_name = target.task.name

        event = RunStarted(
            run_id=run_id, target_tasks=[target_name], params=params or {}
        )
        self.bus.publish(event)

        # ExitStack manages the teardown of resources
        with ExitStack() as stack:
            try:
                # 1. Build graph to discover resources
                # We need to build the graph once here to find global resources for this run.
                # Note: Sub-graphs generated by .map() might need resources not discovered here if they depend on
                # parameters that change resources. But typically resources are static.
                # For dynamic resources, we might need a more complex strategy.
                # For now, we assume top-level graph building reveals all needed resources for the initial plan.
                initial_graph = build_graph(target)
                initial_plan = self.solver.resolve(initial_graph)

                required_resources = self._scan_for_resources(initial_plan)
                active_resources = self._setup_resources(
                    required_resources, stack, run_id
                )

                # 2. Execute
                final_result = await self._execute_graph(
                    target, params or {}, active_resources, run_id
                )

                run_duration = time.time() - start_time
                final_event = RunFinished(
                    run_id=run_id, status="Succeeded", duration=run_duration
                )
                self.bus.publish(final_event)

                return final_result

            except Exception as e:
                run_duration = time.time() - start_time
                final_fail_event = RunFinished(
                    run_id=run_id,
                    status="Failed",
                    duration=run_duration,
                    error=f"{type(e).__name__}: {e}",
                )
                self.bus.publish(final_fail_event)
                raise

    async def _execute_graph(
        self,
        target: Any,
        params: Dict[str, Any],
        active_resources: Dict[str, Any],
        run_id: str,
    ) -> Any:
        """
        Executes a dependency graph for a given target.
        Can be called recursively for sub-graphs (e.g. inside .map()).
        """
        graph = build_graph(target)
        plan = self.solver.resolve(graph)

        results: Dict[str, Any] = {}
        skipped_node_ids: set[str] = set()

        self._inject_params(plan, params, results)

        for node in plan:
            if node.node_type == "param":
                continue

            skip_reason = self._should_skip(node, graph, results, skipped_node_ids)

            if skip_reason:
                skipped_node_ids.add(node.id)
                self.bus.publish(
                    TaskSkipped(
                        run_id=run_id,
                        task_id=node.id,
                        task_name=node.name,
                        reason=skip_reason,
                    )
                )
                continue

            results[node.id] = await self._execute_node_with_policies(
                node, graph, results, active_resources, run_id, params
            )

        return results[target._uuid]

    async def _execute_node_with_policies(
        self,
        node: Node,
        graph: Graph,
        upstream_results: Dict[str, Any],
        active_resources: Dict[str, Any],
        run_id: str,
        params: Dict[str, Any],
    ) -> Any:
        # Resolve resource requirements
        requirements = self._resolve_constraints(node, graph, upstream_results)
        
        # Acquire resources (this may block)
        await self.resource_manager.acquire(requirements)
        
        try:
            return await self._execute_node_internal(
                node, graph, upstream_results, active_resources, run_id, params, requirements
            )
        finally:
            # Always release resources
            await self.resource_manager.release(requirements)

    def _resolve_constraints(
        self, node: Node, graph: Graph, upstream_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Resolves dynamic constraints to concrete values."""
        if not node.constraints or node.constraints.is_empty():
            return {}

        resolved = {}
        for res, amount in node.constraints.requirements.items():
            if isinstance(amount, (LazyResult, MappedLazyResult)):
                # Find the upstream source for this dynamic constraint using the special edge prefix
                # We can also lookup by ID since we know it's a LazyResult, but edge lookup is safer graph-wise.
                # Actually, simply using the result of the LazyResult._uuid is enough because
                # the graph builder ensures it's in upstream_results.
                if amount._uuid in upstream_results:
                    resolved[res] = upstream_results[amount._uuid]
                else:
                    # Depending on skip logic, this might be missing. 
                    # If upstream was skipped, we probably shouldn't be here, 
                    # or we should fail because we can't determine resource usage.
                    raise RuntimeError(
                        f"Could not resolve dynamic resource constraint '{res}' for task '{node.name}'. "
                        "Upstream dependency result missing."
                    )
            else:
                resolved[res] = amount
        return resolved

    async def _execute_node_internal(
        self,
        node: Node,
        graph: Graph,
        upstream_results: Dict[str, Any],
        active_resources: Dict[str, Any],
        run_id: str,
        params: Dict[str, Any],
        requirements: Dict[str, Any], # Passed for logging if needed
    ) -> Any:
        task_start_time = time.time()

        # 0. Check Cache
        if node.cache_policy:
            inputs_for_cache = self._resolve_inputs(node, graph, upstream_results)
            cached_value = node.cache_policy.check(node.id, inputs_for_cache)
            if cached_value is not None:
                self.bus.publish(
                    TaskSkipped(
                        run_id=run_id,
                        task_id=node.id,
                        task_name=node.name,
                        reason="CacheHit",
                    )
                )
                return cached_value

        start_event = TaskExecutionStarted(
            run_id=run_id, task_id=node.id, task_name=node.name
        )
        self.bus.publish(start_event)

        # Special handling for Map Nodes
        if node.node_type == "map":
            try:
                result = await self._execute_map_node(
                    node, graph, upstream_results, active_resources, run_id, params
                )

                task_duration = time.time() - task_start_time
                self.bus.publish(
                    TaskExecutionFinished(
                        run_id=run_id,
                        task_id=node.id,
                        task_name=node.name,
                        status="Succeeded",
                        duration=task_duration,
                        result_preview=f"List[{len(result)} items]",
                    )
                )
                return result
            except Exception as e:
                task_duration = time.time() - task_start_time
                self.bus.publish(
                    TaskExecutionFinished(
                        run_id=run_id,
                        task_id=node.id,
                        task_name=node.name,
                        status="Failed",
                        duration=task_duration,
                        error=str(e),
                    )
                )
                raise e

        # Determine retry policy
        retry_policy = node.retry_policy
        max_attempts = 1 + (retry_policy.max_attempts if retry_policy else 0)
        delay = retry_policy.delay if retry_policy else 0.0
        backoff = retry_policy.backoff if retry_policy else 1.0

        attempt = 0
        last_exception = None

        while attempt < max_attempts:
            attempt += 1
            try:
                result = await self.executor.execute(
                    node, graph, upstream_results, active_resources
                )

                task_duration = time.time() - task_start_time
                finish_event = TaskExecutionFinished(
                    run_id=run_id,
                    task_id=node.id,
                    task_name=node.name,
                    status="Succeeded",
                    duration=task_duration,
                    result_preview=repr(result)[:100],
                )
                self.bus.publish(finish_event)

                # Save to cache if policy exists
                if node.cache_policy:
                    inputs_for_save = self._resolve_inputs(
                        node, graph, upstream_results
                    )
                    node.cache_policy.save(node.id, inputs_for_save, result)

                return result

            except Exception as e:
                last_exception = e
                # If we have retries left, wait and continue
                if attempt < max_attempts:
                    self.bus.publish(
                        TaskRetrying(
                            run_id=run_id,
                            task_id=node.id,
                            task_name=node.name,
                            attempt=attempt,
                            max_attempts=max_attempts,
                            delay=delay,
                            error=str(e),
                        )
                    )
                    await asyncio.sleep(delay)
                    delay *= backoff
                else:
                    # Final failure
                    task_duration = time.time() - task_start_time
                    fail_event = TaskExecutionFinished(
                        run_id=run_id,
                        task_id=node.id,
                        task_name=node.name,
                        status="Failed",
                        duration=task_duration,
                        error=f"{type(e).__name__}: {e}",
                    )
                    self.bus.publish(fail_event)
                    raise last_exception

        # Should not be reached if logic is correct
        raise RuntimeError("Unexpected execution state")

    def _resolve_inputs(
        self, node: Node, graph: Graph, upstream_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Helper to resolve inputs for cache checking."""
        inputs = {}
        incoming_edges = [edge for edge in graph.edges if edge.target.id == node.id]
        for edge in incoming_edges:
            inputs[edge.arg_name] = upstream_results[edge.source.id]
        return inputs

    def _scan_for_resources(self, plan: list[Node]) -> set[str]:
        """Finds all unique resource names required by the plan."""
        required = set()
        for node in plan:
            # 1. Check literal inputs for dynamic injection
            for value in node.literal_inputs.values():
                if isinstance(value, Inject):
                    required.add(value.resource_name)

            # 2. Check function signature for static injection
            # Skip nodes that don't have a callable (e.g., Param nodes)
            if node.callable_obj is None:
                continue

            sig = inspect.signature(node.callable_obj)
            for param in sig.parameters.values():
                if isinstance(param.default, Inject):
                    required.add(param.default.resource_name)
        return required

    def _setup_resources(
        self, required_names: set[str], stack: ExitStack, run_id: str
    ) -> Dict[str, Any]:
        """
        Initializes all required resources and their dependencies recursively.
        Returns a dictionary of active resource instances.
        """
        active: Dict[str, Any] = {}

        def get_or_create(name: str):
            if name in active:
                return active[name]

            provider = self._resource_providers.get(name)
            if not provider:
                raise NameError(f"Resource '{name}' is required but not registered.")

            # Inspect provider's signature to find its dependencies
            sig = inspect.signature(provider)
            deps = {}
            for param_name, param in sig.parameters.items():
                if isinstance(param.default, Inject):
                    deps[param_name] = get_or_create(param.default.resource_name)
                # Here you could also inject `Param`s if needed

            # Create the resource generator
            gen = provider(**deps)

            # Enter the generator and get the yielded value
            instance = next(gen)
            active[name] = instance
            self.bus.publish(ResourceAcquired(run_id=run_id, resource_name=name))

            # Register the teardown logic
            stack.callback(self._teardown_resource, gen, run_id, name)

            return instance

        for name in required_names:
            get_or_create(name)

        return active

    async def _execute_map_node(
        self,
        node: Node,
        graph: Graph,
        upstream_results: Dict[str, Any],
        active_resources: Dict[str, Any],
        run_id: str,
        params: Dict[str, Any],
    ) -> List[Any]:
        """
        Dynamically unfolds and executes a map node.
        """
        # 1. Resolve inputs
        # Start with static literals
        mapped_inputs = node.literal_inputs.copy()

        # Merge dynamic inputs from upstream
        dynamic_inputs = self._resolve_inputs(node, graph, upstream_results)
        mapped_inputs.update(dynamic_inputs)

        if not mapped_inputs:
            return []

        # 2. Validate input lengths
        lengths = {k: len(v) for k, v in mapped_inputs.items()}
        first_len = list(lengths.values())[0]
        if not all(l == first_len for l in lengths.values()):
            raise ValueError(f"Mapped inputs have mismatched lengths: {lengths}")

        # 3. Generate sub-tasks
        # We invoke the factory for each item to get a LazyResult (a sub-graph root)
        sub_targets = []
        factory = node.mapping_factory

        for i in range(first_len):
            # Extract the i-th item from each iterable input
            kwargs_for_item = {k: v[i] for k, v in mapped_inputs.items()}
            # Invoke factory to get the LazyResult
            # Task objects implement __call__ to create LazyResult
            sub_target = factory(**kwargs_for_item)
            sub_targets.append(sub_target)

        # 4. Execute sub-tasks in parallel
        # We recursively call _execute_graph for each sub-target.
        # This supports sub-targets being complex graphs themselves.
        # We share active_resources with them.

        # Note: factory(**kwargs) calls the Task.__call__ which returns a LazyResult.
        # Task class does NOT implement map_item, it implements __call__.
        # So factory(**kwargs_for_item) is correct for Task objects.

        coros = [
            self._execute_graph(target, params, active_resources, run_id)
            for target in sub_targets
        ]

        return await asyncio.gather(*coros)

    def _teardown_resource(self, gen: Generator, run_id: str, resource_name: str):
        """Helper to exhaust a resource generator for cleanup."""
        try:
            next(gen)
        except StopIteration:
            self.bus.publish(
                ResourceReleased(run_id=run_id, resource_name=resource_name)
            )
