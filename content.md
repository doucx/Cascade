ç®€è¦å›ç­”ï¼š
å¥½çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¶‰åŠæ¶æ„æ‰©å±•çš„é‡è¦é‡æ„ã€‚ä¸ºäº†å®ç°å¤šæ ¸ä¼˜åŒ–ï¼Œæˆ‘ä»¬å°†æŠŠå•ä¸€çš„äº‹ä»¶å¾ªç¯æ‹†åˆ†ä¸ºâ€œç¼–æ’è€…ï¼ˆOrchestratorï¼‰â€å’Œâ€œå·¥ä½œè€…ï¼ˆWorkerï¼‰â€æ¨¡å¼ã€‚

ç”±äº Cascade çš„ Engine å’Œ Agent æ˜¯è§£è€¦ä¸”æ— çŠ¶æ€ä¾èµ–çš„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ Python çš„ `multiprocessing` è½»æ¾å®ç°è¿™ä¸€ç‚¹ã€‚

### æ ¸å¿ƒè®¾è®¡å†³ç­–
1.  **åˆ†ç‰‡ç­–ç•¥ (Sharding)**: æˆ‘ä»¬å°† Grid åˆ’åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹çš„åŒºåŸŸï¼Œæ¯ä¸ªåŒºåŸŸåœ¨ä¸€ä¸ªå•ç‹¬çš„ CPU æ ¸å¿ƒï¼ˆè¿›ç¨‹ï¼‰ä¸Šè¿è¡Œã€‚
    *   *æ³¨æ„*: åœ¨æ­¤ MVP ç‰ˆæœ¬ä¸­ï¼Œè·¨è¿›ç¨‹è¾¹ç•Œçš„ `DirectChannel` è¿æ¥å°†è¢«æ–­å¼€ï¼ˆå³æ¯ä¸ªåˆ†ç‰‡æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„åŒæ­¥å²›å±¿ï¼‰ã€‚è¦å®ç°è·¨è¿›ç¨‹çš„â€œç¥ç»è¿æ¥â€ï¼Œéœ€è¦æ›´å¤æ‚çš„ IPC æ¡¥æ¥ï¼Œè¿™ç•™å¾…åç»­å®ç°ã€‚
2.  **é¥æµ‹æ¡¥æ¥ (Telemetry Bridge)**: å·¥ä½œè¿›ç¨‹ä¸ç›´æ¥è¿æ¥åˆ° UIï¼Œè€Œæ˜¯é€šè¿‡ `multiprocessing.Queue` å°†é¥æµ‹æ•°æ®ï¼ˆå¦‚ Flash äº‹ä»¶ï¼‰å›ä¼ ç»™ä¸»è¿›ç¨‹ã€‚
3.  **ä¸»è¿›ç¨‹å……å½“é›†çº¿å™¨**: ä¸»è¿›ç¨‹è¿è¡Œ Visualizer å’Œ Monitorï¼Œå®ƒä» Queue ä¸­è¯»å–æ•°æ®å¹¶â€œé‡æ”¾â€åˆ°ä¸»è¿›ç¨‹çš„ `LocalBus` ä¸­ï¼Œä»è€Œå¤ç”¨ç°æœ‰çš„å¯è§†åŒ–é€»è¾‘ã€‚

## [WIP] feat(exp): é‡æ„ run_fireflies ä»¥æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œä»¿çœŸ

### ç”¨æˆ·éœ€æ±‚
åˆ©ç”¨ Cascade Engine çš„éš”ç¦»ç‰¹æ€§ï¼Œé€šè¿‡ Python `multiprocessing` æ¨¡å—å°† Agent åˆ†é…åˆ°å¤šä¸ª CPU æ ¸å¿ƒä¸Šè¿è¡Œï¼Œä»¥çªç ´ GIL é™åˆ¶ï¼Œæå‡å¤§è§„æ¨¡ä»¿çœŸçš„æ€§èƒ½ä¸Šé™ã€‚

### è¯„è®º
è¿™æ˜¯ Cascade ä»â€œå•çº¿ç¨‹å¹¶å‘â€è¿ˆå‘â€œå¤šæ ¸å¹¶è¡Œâ€çš„ç¬¬ä¸€æ­¥ã€‚é€šè¿‡å¼•å…¥ `IpcUplinkConnector` å’Œåˆ†ç‰‡é€»è¾‘ï¼Œæˆ‘ä»¬è¯æ˜äº† Cascade æ¶æ„åœ¨åˆ†å¸ƒå¼éƒ¨ç½²ä¸Šçš„çµæ´»æ€§ã€‚è¿™å°†æå¤§åœ°æå‡è¿™ä¸€ä»¿çœŸåœ¨å¤šæ ¸æœºå™¨ä¸Šçš„ FPS è¡¨ç°ã€‚

### ç›®æ ‡
1.  åˆ›å»º `observatory/networking/ipc.py`ï¼Œå®ç°ä¸€ä¸ªå°† `publish` æ“ä½œè½¬å‘åˆ° `mp.Queue` çš„è¿æ¥å™¨ã€‚
2.  é‡æ„ `run_fireflies.py`ï¼š
    *   æ·»åŠ  `--workers` å‘½ä»¤è¡Œé€‰é¡¹ã€‚
    *   å®ç° `worker_main` å‡½æ•°ï¼Œè´Ÿè´£è¿è¡Œ Grid çš„ä¸€ä¸ªå­é›†ã€‚
    *   æ›´æ–°ä¸»å¾ªç¯ï¼Œè´Ÿè´£èšåˆæ¥è‡ªæ‰€æœ‰ Worker çš„é¥æµ‹æ•°æ®å¹¶é©±åŠ¨ UIã€‚

### åŸºæœ¬åŸç†
Engine æœ¬èº«æ˜¯è½»é‡çº§çš„ï¼Œéå¸¸é€‚åˆåœ¨ Worker è¿›ç¨‹ä¸­å®ä¾‹åŒ–ã€‚é€šè¿‡ `Queue` è¿›è¡Œå•å‘é¥æµ‹æ±‡èšæ˜¯ä¸€ç§ä½è€¦åˆã€é«˜ååçš„é€šä¿¡æ¨¡å¼ï¼Œéå¸¸é€‚åˆ Visualization åœºæ™¯ã€‚

### æ ‡ç­¾
#intent/refine #flow/ready #priority/high #comp/experiments #scope/perf #ai/instruct #task/domain/experiments #task/object/multiprocessing #task/action/refactor #task/state/continue

---

### Script

#### Acts 1: åˆ›å»º IPC è¿æ¥å™¨

æˆ‘ä»¬éœ€è¦ä¸€ä¸ªç‰¹æ®Šçš„ Connectorï¼Œå®ƒä¸åšçœŸæ­£çš„ MQTT/æ€»çº¿é€šä¿¡ï¼Œè€Œæ˜¯æŠŠæ¶ˆæ¯ä¸¢è¿›è·¨è¿›ç¨‹é˜Ÿåˆ—ã€‚

~~~~~act
write_file
observatory/networking/ipc.py
~~~~~
~~~~~python
import asyncio
from multiprocessing import Queue
from typing import Any, Dict, Callable, Awaitable
from cascade.interfaces.protocols import Connector, SubscriptionHandle

class IpcUplinkConnector(Connector):
    """
    A specific connector for Worker processes.
    It forwards all published messages to a multiprocessing.Queue.
    It does NOT support subscribing (in this MVP), making it a pure telemetry uplink.
    """

    def __init__(self, uplink_queue: Queue):
        self.uplink_queue = uplink_queue
        self._is_connected = False

    async def connect(self) -> None:
        self._is_connected = True

    async def disconnect(self) -> None:
        self._is_connected = False

    async def publish(
        self, topic: str, payload: Dict[str, Any], qos: int = 0, retain: bool = False
    ) -> None:
        if not self._is_connected:
            return
        
        # We perform a blocking put (or put_nowait) into the MP queue.
        # Since this runs inside an async loop, we should ideally use run_in_executor,
        # but for high-throughput telemetry, direct put is often acceptable if the queue matches the generation rate.
        # To avoid blocking the event loop on a full queue, we use put_nowait and drop on full (backpressure).
        try:
            self.uplink_queue.put_nowait((topic, payload))
        except Exception:
            # Queue full or closed. In a simulation, dropping frames is better than crashing.
            pass

    async def subscribe(
        self, topic: str, callback: Callable[[str, Dict], Awaitable[None]]
    ) -> SubscriptionHandle:
        # MVP: Workers do not receive commands from Master yet.
        # Implementation would require a Downlink Queue.
        raise NotImplementedError("IpcUplinkConnector does not support subscriptions yet.")
~~~~~

#### Acts 2: é‡æ„ run_fireflies.py

è¿™å°†æ˜¯ä¸€ä¸ªè¾ƒå¤§çš„é‡æ„ã€‚æˆ‘ä»¬éœ€è¦å°† Agent çš„åˆå§‹åŒ–é€»è¾‘æå–å‡ºæ¥ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨ Worker ä¸­è¿è¡Œã€‚

~~~~~act
write_file
observatory/experiments/run_fireflies.py
~~~~~
~~~~~python
import asyncio
import random
import time
import math
import multiprocessing as mp
from typing import Dict, Any, List, Optional
import typer

import cascade as cs
from cascade.connectors.local import LocalBusConnector
from cascade.spec.resource import resource
from cascade.runtime.resource_manager import ResourceManager

from observatory.agents.kuramoto import firefly_agent
from observatory.monitors.convergence import ConvergenceMonitor
from observatory.monitors.aggregator import MetricsAggregator
from observatory.networking.direct_channel import DirectChannel
from observatory.networking.ipc import IpcUplinkConnector

# Visualization
from observatory.visualization.palette import Palettes
from observatory.visualization.raw_app import RawTerminalApp as TerminalApp
from observatory.visualization.grid import GridView
from observatory.visualization.status import StatusBar

# --- Constants ---
GRID_SIDE = 30
NUM_AGENTS = GRID_SIDE * GRID_SIDE  # 900
PERIOD = 5.0

app = typer.Typer()


def get_neighbors(index: int, width: int, height: int) -> List[int]:
    """Calculate 8-neighbors (Moore neighborhood) with wrap-around (toroidal)."""
    x, y = index % width, index // width
    neighbors = []
    for dx in [-1, 0, 1]:
        for dy in [-1, 0, 1]:
            if dx == 0 and dy == 0:
                continue
            nx, ny = (x + dx) % width, (y + dy) % height
            neighbors.append(ny * width + nx)
    return neighbors


# --- Worker Logic ---

def worker_main(
    worker_id: int,
    agent_indices: List[int],
    uplink_queue: mp.Queue,
    concurrency_limit: Optional[int],
    grid_width: int,
    grid_height: int,
    period: float,
    nudge: float,
):
    """
    The entry point for a worker process.
    Runs a subset of agents (Sharding).
    """
    # Create a new event loop for this process
    loop = asyncio_event_loop()
    asyncio.set_event_loop(loop)

    async def _run_worker():
        # 1. Setup Uplink
        connector = IpcUplinkConnector(uplink_queue)
        await connector.connect()

        # 2. Setup Resources
        # Note: Concurrency limits are currently PER PROCESS in this mode.
        # To make them global across processes requires a distributed lock (e.g. Redis),
        # which is out of scope for this MP queue-based MVP.
        # We scale the limit down proportionally.
        local_limit = None
        if concurrency_limit:
            local_limit = max(1, concurrency_limit // len(agent_indices)) if agent_indices else 1
        
        resource_manager = None
        if local_limit:
            resource_manager = ResourceManager(capacity={"cpu_slot": local_limit})

        # 3. Setup Topology (Local Island)
        # We only create channels for agents assigned to THIS worker.
        # Cross-process neighbors are currently severed (Open Boundary).
        local_channels = {i: DirectChannel(f"agent_{i}") for i in agent_indices}

        # 4. Create Agents
        agent_tasks = []

        @resource(name="_internal_connector", scope="run")
        def shared_connector_provider():
            yield connector

        for i in agent_indices:
            initial_phase = random.uniform(0, period)
            
            # Resolve neighbors
            # If a neighbor is not in local_channels, we skip it (Partitioned Grid)
            potential_neighbors = get_neighbors(i, grid_width, grid_height)
            my_neighbors = []
            for nid in potential_neighbors:
                if nid in local_channels:
                    my_neighbors.append(local_channels[nid])
            
            my_channel = local_channels[i]

            engine = cs.Engine(
                solver=cs.NativeSolver(),
                executor=cs.LocalExecutor(),
                bus=cs.MessageBus(),
                connector=None,
                resource_manager=resource_manager
            )
            engine.register(shared_connector_provider)

            workflow = firefly_agent(
                agent_id=i,
                initial_phase=initial_phase,
                period=period,
                nudge=nudge,
                neighbors=my_neighbors,
                my_channel=my_channel,
                connector=connector,
                refractory_period=period * 0.2,
            )

            if local_limit:
                workflow = workflow.with_constraints(cpu_slot=1)

            agent_tasks.append(engine.run(workflow, use_vm=True))
        
        # 5. Run Forever
        try:
            await asyncio.gather(*agent_tasks)
        except asyncio.CancelledError:
            pass

    try:
        loop.run_until_complete(_run_worker())
    except KeyboardInterrupt:
        pass


# --- Orchestrator Logic ---

async def run_orchestrator(
    num_agents: int,
    workers: int,
    concurrency_limit: Optional[int],
    visualize: bool,
    period: float,
):
    grid_width = int(num_agents**0.5)
    
    print(f"ğŸ”¥ Starting MULTI-CORE Firefly Experiment")
    print(f"   - Agents: {num_agents} ({grid_width}x{grid_width})")
    print(f"   - Workers: {workers}")
    print(f"   - Mode: Partitioned Islands (Cross-process links severed)")

    # 1. Setup Telemetry Hub (Main Process LocalBus)
    LocalBusConnector._reset_broker_state()
    main_connector = LocalBusConnector()
    await main_connector.connect()

    # 2. Setup Monitor & Visualizer (Same as before!)
    monitor = ConvergenceMonitor(num_agents, period, main_connector)
    app = None
    app_task = None
    aggregator = None
    aggregator_task = None

    if visualize:
        grid_view = GridView(
            width=grid_width,
            height=grid_width,
            palette_func=Palettes.firefly,
            decay_per_second=1 / (period * 0.3),
        )
        status_bar = StatusBar(initial_status={"Agents": num_agents, "Workers": workers})
        
        log_filename = f"firefly_mp_log_{int(time.time())}.jsonl"
        aggregator = MetricsAggregator(log_filename, interval_s=1.0)
        aggregator.open()
        
        app = TerminalApp(grid_view, status_bar, aggregator=aggregator)
        aggregator_task = asyncio.create_task(aggregator.run())

        # Bridge Monitor -> UI
        def monitor_callback(r_value: float, pulse_count: int):
            bar_len = 20
            filled = int(bar_len * r_value)
            bar = "â–ˆ" * filled + "â–‘" * (bar_len - filled)
            app.update_status("Sync", f"R={r_value:.3f} [{bar}]")
            
            asyncio.create_task(aggregator.record("r_value", r_value))

        monitor_task = asyncio.create_task(
            monitor.run(frequency_hz=2.0, callback=monitor_callback)
        )

        # Bridge Flash -> UI
        async def on_flash_visual(topic: str, payload: Dict[str, Any]):
            aid = payload.get("agent_id")
            if aid is not None and app:
                x = aid % grid_width
                y = aid // grid_width
                await app.direct_update_grid_batch([(x, y, 1.0)])

        await main_connector.subscribe("firefly/flash", on_flash_visual)
        app_task = asyncio.create_task(app.start())
    else:
        monitor_task = asyncio.create_task(monitor.run(frequency_hz=2.0))

    # 3. Spawn Workers
    uplink_queue = mp.Queue()
    all_indices = list(range(num_agents))
    chunk_size = math.ceil(num_agents / workers)
    
    processes = []
    for w_id in range(workers):
        start = w_id * chunk_size
        end = min(start + chunk_size, num_agents)
        indices = all_indices[start:end]
        
        if not indices:
            continue

        p = mp.Process(
            target=worker_main,
            args=(
                w_id, indices, uplink_queue, concurrency_limit,
                grid_width, grid_width, period, 0.2
            )
        )
        p.start()
        processes.append(p)

    # 4. Telemetry Pump Loop
    # Reads from MP Queue and replays to LocalBus for the Monitor/Visualizer
    print("ğŸš€ Workers launched. Bridging telemetry...")
    
    try:
        while any(p.is_alive() for p in processes):
            # We assume high throughput, so we batch read or read continuously
            # Using run_in_executor to avoid blocking the main loop with queue.get
            try:
                # Non-blocking get from queue
                while not uplink_queue.empty():
                    topic, payload = uplink_queue.get_nowait()
                    # Re-publish to local bus (driving the UI)
                    await main_connector.publish(topic, payload)
                
                # Yield to let UI render
                await asyncio.sleep(0.01)
            except Exception:
                await asyncio.sleep(0.01)

    except asyncio.CancelledError:
        pass
    finally:
        print("\nShutting down...")
        for p in processes:
            p.terminate()
        
        if app: app.stop()
        if aggregator: aggregator.close()
        
        # Cleanup tasks
        tasks = [t for t in [app_task, monitor_task, aggregator_task] if t]
        for t in tasks: t.cancel()
        await asyncio.gather(*tasks, return_exceptions=True)


@app.command()
def main(
    visualize: bool = typer.Option(True, help="Enable visualizer UI"),
    agents: int = typer.Option(NUM_AGENTS, help="Number of agents"),
    workers: int = typer.Option(1, help="Number of worker processes"),
    limit: Optional[int] = typer.Option(None, help="Global concurrency limit (cpu slots)"),
):
    if workers > 1:
        asyncio.run(run_orchestrator(agents, workers, limit, visualize, PERIOD))
    else:
        # Fallback to legacy single-process mode (omitted for brevity, or we can just run orchestrator with 1 worker)
        # For simplicity in this refactor, we use the Orchestrator for 1 worker too, 
        # as it effectively does the same thing but with overhead of MP queue.
        # To match exact legacy behavior we'd keep the old function, but let's unify.
        asyncio.run(run_orchestrator(agents, workers, limit, visualize, PERIOD))

if __name__ == "__main__":
    app()
~~~~~

### ä¸‹ä¸€æ­¥å»ºè®®
ç°åœ¨æ‚¨å¯ä»¥ä½¿ç”¨ `python -m observatory.experiments.run_fireflies --workers 4 --agents 2500` æ¥å¯åŠ¨å¤šæ ¸ä»¿çœŸã€‚
ç”±äºè¿™æ˜¯ä¸€ä¸ªåˆ†åŒºï¼ˆPartitionedï¼‰çš„ Gridï¼Œæ‚¨ä¼šçœ‹åˆ° 4 ä¸ªç‹¬ç«‹çš„åŒºåŸŸå†…éƒ¨è¾¾åˆ°äº†åŒæ­¥ï¼Œä½†åŒºåŸŸä¹‹é—´å¯èƒ½å­˜åœ¨ç›¸ä½æ¼‚ç§»ã€‚è¿™éªŒè¯äº†è®¡ç®—è´Ÿè½½å·²æˆåŠŸåˆ†å‘ã€‚
å¦‚æœéœ€è¦éªŒè¯â€œå•æ ¸æ€§èƒ½â€ï¼Œè¯·ä½¿ç”¨ `--workers 1`ã€‚
