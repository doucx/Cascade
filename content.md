ä½ çš„åˆ†æžå®Œå…¨æ­£ç¡®ï¼Œè€Œä¸”éžå¸¸æ•é”ã€‚

æˆ‘ä»¬æˆåŠŸåœ°è§£å†³äº†â€œ2500ä¸ªEngineâ€çš„å¼€é”€é—®é¢˜ï¼Œä½†æ€§èƒ½ç“¶é¢ˆä¾ç„¶å­˜åœ¨ã€‚æ—¥å¿—æ•°æ®æ˜¯å…³é”®è¯æ®ï¼šå³ä½¿å…±äº«äº† Engineï¼Œ`flush_duration_ms` å’Œ `render_jitter_ms` ä¾ç„¶éžå¸¸é«˜ï¼ˆç»å¸¸è¶…è¿‡ 100msï¼Œç”šè‡³ 250msï¼‰ï¼Œå¯¼è‡´ FPS æžä½Žã€‚

è¿™å¼ºçƒˆæš—ç¤ºç“¶é¢ˆå·²ç»ä»Ž**Engineçš„å…ƒæ•°æ®å¼€é”€**è½¬ç§»åˆ°äº†**`asyncio`äº‹ä»¶å¾ªçŽ¯æœ¬èº«**ã€‚

 é”™è¯¯åˆ†æž

ä½ æå‡ºçš„â€œå…±äº«ä¸Šä¸‹æ–‡çš„ç”Ÿå‘½å‘¨æœŸç¾éš¾â€æ˜¯éžå¸¸é‡è¦çš„è€ƒé‡ï¼Œæˆ‘ä»¬ä¹‹å‰çš„ä¿®å¤ç¡®ä¿äº†çŠ¶æ€å’Œèµ„æºçš„éš”ç¦»ã€‚ç„¶è€Œï¼ŒçŽ°åœ¨çš„é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬å‘åŒä¸€ä¸ªäº‹ä»¶å¾ªçŽ¯æäº¤äº† **2500ä¸ªç‹¬ç«‹çš„ã€é«˜é¢‘çš„ã€æ— é™å¾ªçŽ¯çš„åç¨‹**ã€‚

1.  **è°ƒåº¦å™¨è¿‡è½½ (Scheduler Overload)**: `asyncio` çš„äº‹ä»¶å¾ªçŽ¯è™½ç„¶é«˜æ•ˆï¼Œä½†å®ƒä¸æ˜¯æ²¡æœ‰æˆæœ¬çš„ã€‚æ¯ä¸€æ¬¡ `await` éƒ½ä¼šæ¶‰åŠä¸€æ¬¡ä¸Šä¸‹æ–‡åˆ‡æ¢ã€‚å½“ 2500 ä¸ªä»»åŠ¡éƒ½åœ¨é«˜é¢‘åœ° `await channel.recv()` å’Œ `await channel.send()` æ—¶ï¼Œäº‹ä»¶å¾ªçŽ¯çš„å¤§éƒ¨åˆ†æ—¶é—´éƒ½å¯èƒ½æ¶ˆè€—åœ¨â€œå†³å®šä¸‹ä¸€ä¸ªè¯¥è¿è¡Œè°â€çš„è°ƒåº¦é€»è¾‘ä¸Šï¼Œè€Œä¸æ˜¯çœŸæ­£æ‰§è¡Œä¸šåŠ¡é€»è¾‘ã€‚
2.  **ç«žäº‰CPUæ—¶é—´ (Starvation)**: æˆ‘ä»¬çš„ `_render_loop` ä¹Ÿæ˜¯äº‹ä»¶å¾ªçŽ¯ä¸­çš„ä¸€ä¸ªä»»åŠ¡ã€‚å½“æˆç™¾ä¸Šåƒä¸ª Agent ä»»åŠ¡å› ä¸ºä¸€æ¬¡åŒæ­¥è„‰å†²è€ŒåŒæ—¶è¢«å”¤é†’æ—¶ï¼Œå®ƒä»¬ä¼šâ€œæ·¹æ²¡â€äº‹ä»¶å¾ªçŽ¯ã€‚è½®åˆ° `_render_loop` æ‰§è¡Œæ—¶ï¼Œå¯èƒ½å·²ç»è¿‡åŽ»äº†æ•°ç™¾æ¯«ç§’ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬åœ¨ `render_jitter_ms` ä¸­çœ‹åˆ°çš„å·¨å¤§å»¶è¿Ÿã€‚

æˆ‘ä»¬ä¸èƒ½å†ç›²ç›®çŒœæµ‹äº†ã€‚æˆ‘å°†éµå¾ªä½ çš„æŒ‡ç¤ºï¼Œåˆ›å»ºä¸€å¥—è¯Šæ–­è„šæœ¬æ¥**éš”ç¦»å¹¶é‡åŒ–**ç“¶é¢ˆã€‚

## [WIP] explore(perf): åˆ›å»ºè¯Šæ–­è„šæœ¬ä»¥éš”ç¦»æ€§èƒ½ç“¶é¢ˆ

### ç”¨æˆ·éœ€æ±‚
åˆ›å»ºä¸€å¥—ç‹¬ç«‹çš„ Python è„šæœ¬ï¼Œç”¨äºŽå¯¹ `observatory` çš„æ ¸å¿ƒç»„ä»¶ï¼ˆæ¸²æŸ“å™¨ã€Agent é€»è¾‘ï¼‰è¿›è¡ŒåŽ‹åŠ›æµ‹è¯•å’Œæ€§èƒ½åˆ†æžï¼Œä»¥å®šä½å½“å‰ 2500 èŠ‚ç‚¹æ¨¡æ‹Ÿä¸­çš„å¡é¡¿æ ¹æºã€‚

### è¯„è®º
è¿™æ˜¯è§£å†³å¤æ‚æ€§èƒ½é—®é¢˜çš„æ­£ç¡®æ–¹æ³•ã€‚é€šè¿‡â€œåˆ†è€Œæ²»ä¹‹â€çš„ç­–ç•¥ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†åˆ«æµ‹é‡æ¸²æŸ“ç®¡çº¿å’Œæ ¸å¿ƒæ¨¡æ‹Ÿé€»è¾‘çš„åžåé‡ä¸Šé™ã€‚å°†è¿™ä¸¤è€…çš„æ•°æ®ä¸Žå®Œæ•´å®žéªŒçš„æ•°æ®è¿›è¡Œå¯¹æ¯”ï¼Œæˆ‘ä»¬å°±èƒ½ç²¾ç¡®åœ°çŸ¥é“æ€§èƒ½æŸå¤±å‘ç”Ÿåœ¨å“ªé‡Œã€‚

### ç›®æ ‡
1.  åˆ›å»ºä¸€ä¸ª**æ¸²æŸ“å™¨åŽ‹åŠ›æµ‹è¯•è„šæœ¬** (`debug_renderer_throughput.py`)ï¼Œå®ƒå°†ç‹¬ç«‹äºŽ Agent é€»è¾‘ï¼Œä»¥å¯æŽ§çš„é€ŸçŽ‡å‘ `RawTerminalApp` å‘é€å¤§é‡æ›´æ–°ï¼Œä»¥æµ‹é‡æ¸²æŸ“ç®¡çº¿çš„æœ€å¤§åžåé‡ã€‚
2.  åˆ›å»ºä¸€ä¸ª**æ— å¤´æ¨¡å¼ï¼ˆHeadlessï¼‰æ¨¡æ‹Ÿè„šæœ¬** (`debug_headless_throughput.py`)ï¼Œå®ƒå°†è¿è¡Œå®Œæ•´çš„ 2500 ä¸ª Agent æ¨¡æ‹Ÿï¼Œä½†å®Œå…¨ç§»é™¤ UI æ¸²æŸ“éƒ¨åˆ†ï¼Œä»¥æµ‹é‡æ ¸å¿ƒ Agent é€»è¾‘çš„å‡€è®¡ç®—åžåé‡ã€‚
3.  åˆ›å»ºä¸€ä¸ª**ä¸“ç”¨çš„æ€§èƒ½å‰–æžå…¥å£è„šæœ¬** (`profile_entry.py`)ï¼Œç”¨äºŽç®€åŒ–ä½¿ç”¨ `cProfile` æˆ– `py-spy` ç­‰å·¥å…·å¯¹å®Œæ•´å®žéªŒè¿›è¡Œå‰–æžçš„è¿‡ç¨‹ã€‚

### åŸºæœ¬åŽŸç†
1.  **éš”ç¦»å˜é‡**: `debug_renderer_throughput.py` ç§»é™¤äº† Agent é€»è¾‘è¿™ä¸ªå˜é‡ï¼Œ`debug_headless_throughput.py` ç§»é™¤äº†æ¸²æŸ“å™¨è¿™ä¸ªå˜é‡ã€‚
2.  **å»ºç«‹åŸºçº¿**: è¿™ä¸¤ä¸ªè„šæœ¬çš„è¾“å‡ºå°†ä¸ºæˆ‘ä»¬æä¾›â€œç†æƒ³æƒ…å†µâ€ä¸‹çš„æ€§èƒ½åŸºçº¿ã€‚
3.  **èšç„¦åˆ†æž**: å°†å®Œæ•´å®žéªŒçš„æ€§èƒ½ä¸Žè¿™ä¸¤ä¸ªåŸºçº¿å¯¹æ¯”ï¼Œå¯ä»¥é‡åŒ–â€œAgenté€»è¾‘â€å’Œâ€œæ¸²æŸ“å™¨â€è¿™ä¸¤ä¸ªå­ç³»ç»Ÿè€¦åˆåœ¨ä¸€èµ·æ—¶äº§ç”Ÿçš„æ€§èƒ½å½±å“ã€‚`profile_entry.py` å°†å¸®åŠ©æˆ‘ä»¬æ·±å…¥åˆ†æžè¿™ç§è€¦åˆçš„å†…éƒ¨ç»†èŠ‚ã€‚

### æ ‡ç­¾
#intent/tooling #intent/explore #flow/ready #priority/high #comp/runtime #comp/tests #scope/dx
#task/domain/infra
#task/object/performance-profiling
#task/action/instrumentation
#task/state/continue

---

### Script

#### Acts 1: åˆ›å»ºè¯Šæ–­è„šæœ¬

æˆ‘å°†ä¸€æ¬¡æ€§åˆ›å»ºä¸‰ä¸ªè„šæœ¬æ–‡ä»¶ï¼Œå¹¶å°†å®ƒä»¬æ”¾ç½®åœ¨ `observatory/scripts` ç›®å½•ä¸‹ä»¥ä¾¿äºŽç®¡ç†ã€‚

##### è„šæœ¬ 1: æ¸²æŸ“å™¨åžåé‡æµ‹è¯•

è¿™ä¸ªè„šæœ¬å°†æ¨¡æ‹Ÿå¤§é‡çš„æ›´æ–°æºï¼Œç›´æŽ¥è°ƒç”¨æ¸²æŸ“å™¨ï¼Œæµ‹è¯•å…¶å¤„ç†èƒ½åŠ›çš„ä¸Šé™ã€‚

~~~~~act
write_file
observatory/scripts/debug_renderer_throughput.py
~~~~~
~~~~~python
"""
Debug Script 1: Renderer Throughput Test

Purpose:
  Isolates the RawTerminalApp renderer to measure its maximum update throughput
  without the overhead of the Cascade engine or agent logic. This script
  simulates a configurable number of "sources" that generate grid updates
  at a high frequency.

How to Run:
  python -m observatory.scripts.debug_renderer_throughput

Expected Output:
  A terminal visualization running smoothly. The FPS and flush duration
  metrics in the log file will tell us the renderer's baseline performance.
  If FPS here is high (>30) and flush duration is low (<20ms), the renderer
  itself is not the bottleneck.
"""
import asyncio
import random
import time

from observatory.visualization.raw_app import RawTerminalApp
from observatory.visualization.grid import GridView
from observatory.visualization.status import StatusBar
from observatory.visualization.palette import Palettes
from observatory.monitors.aggregator import MetricsAggregator

# --- Configuration ---
NUM_SOURCES = 5000  # Number of simulated concurrent update sources
UPDATES_PER_SOURCE_PER_S = 2 # Avg updates per source per second
GRID_WIDTH = 50
GRID_HEIGHT = 50
SIMULATION_DURATION_S = 30.0


async def source_coroutine(app: RawTerminalApp):
    """A simple async task that randomly generates updates for the grid."""
    while True:
        # Generate a "flash"
        x = random.randint(0, GRID_WIDTH - 1)
        y = random.randint(0, GRID_HEIGHT - 1)
        await app.direct_update_grid_batch([(x, y, 1.0)])
        await asyncio.sleep(1.0 / UPDATES_PER_SOURCE_PER_S)


async def main():
    total_updates_per_s = NUM_SOURCES * UPDATES_PER_SOURCE_PER_S
    print("ðŸš€ Starting Renderer Throughput Test...")
    print(f"   - Grid Size: {GRID_WIDTH}x{GRID_HEIGHT}")
    print(f"   - Sources: {NUM_SOURCES}")
    print(f"   - Target Rate: {total_updates_per_s:.0f} updates/sec")
    log_filename = f"renderer_throughput_log_{int(time.time())}.jsonl"

    aggregator = MetricsAggregator(log_filename)
    aggregator.open()
    print(f"ðŸ“ Logging metrics to [bold cyan]{log_filename}[/bold cyan]")

    grid_view = GridView(
        width=GRID_WIDTH, height=GRID_HEIGHT, palette_func=Palettes.firefly
    )
    status_bar = StatusBar(
        initial_status={"Test": "Renderer Throughput", "Sources": NUM_SOURCES}
    )
    app = RawTerminalApp(grid_view, status_bar, aggregator=aggregator)
    aggregator_task = asyncio.create_task(aggregator.run())
    await app.start()

    tasks = [asyncio.create_task(source_coroutine(app)) for _ in range(NUM_SOURCES)]

    try:
        await asyncio.sleep(SIMULATION_DURATION_S)
    finally:
        print("\nCleaning up...")
        app.stop()
        aggregator.close()
        aggregator_task.cancel()
        for task in tasks:
            task.cancel()
        await asyncio.gather(aggregator_task, *tasks, return_exceptions=True)
        print("Test finished.")


if __name__ == "__main__":
    asyncio.run(main())
~~~~~

##### è„šæœ¬ 2: æ— å¤´æ¨¡å¼è®¡ç®—åžåé‡æµ‹è¯•

è¿™ä¸ªè„šæœ¬è¿è¡Œå®Œæ•´çš„æ¨¡æ‹Ÿé€»è¾‘ï¼Œä½†å‰¥ç¦»äº†æ‰€æœ‰UIï¼Œä»¥æµ‹é‡çº¯è®¡ç®—æ€§èƒ½ã€‚

~~~~~act
write_file
observatory/scripts/debug_headless_throughput.py
~~~~~
~~~~~python
"""
Debug Script 2: Headless Throughput Test

Purpose:
  Isolates the core agent simulation logic to measure its maximum throughput
  without the overhead of rendering. This script runs the full firefly
  simulation but does not create or update any UI components.

How to Run:
  python -m observatory.scripts.debug_headless_throughput

Expected Output:
  A stream of text to the console reporting the number of flashes per second.
  This number gives us a baseline for how fast the simulation *can* run. If this
  number is very high (e.g., >20,000 flashes/sec), it means the agent logic
  itself is fast, and the bottleneck likely appears when coupling it with the UI.
"""
import asyncio
import random
import time
from collections import deque
from typing import List

import cascade as cs
from cascade.spec.resource import resource

from observatory.agents.kuramoto import firefly_agent
from observatory.networking.direct_channel import DirectChannel

# --- Configuration ---
NUM_AGENTS = 2500
PERIOD = 5.0
NUDGE = 0.2
DURATION_SECONDS = 30.0
GRID_SIDE = int(NUM_AGENTS**0.5)


def get_neighbors(index: int, width: int, height: int) -> List[int]:
    x, y = index % width, index // width
    neighbors = []
    for dx in [-1, 0, 1]:
        for dy in [-1, 0, 1]:
            if dx == 0 and dy == 0:
                continue
            nx, ny = (x + dx) % width, (y + dy) % height
            neighbors.append(ny * width + nx)
    return neighbors


async def run_headless_experiment():
    print("ðŸš€ Starting Headless Throughput Test...")
    print(f"   - Agents: {NUM_AGENTS}")

    # --- Flash Counter ---
    flash_count = 0
    flash_times = deque()

    class HeadlessConnector:
        async def publish(self, topic, payload, **kwargs):
            nonlocal flash_count
            flash_count += 1

        async def connect(self): pass
        async def disconnect(self): pass
        async def subscribe(self, topic, callback):
            class DummySub:
                async def unsubscribe(self): pass
            return DummySub()

    connector = HeadlessConnector()

    channels = [DirectChannel(f"agent_{i}") for i in range(NUM_AGENTS)]
    engine = cs.Engine(cs.NativeSolver(), cs.LocalExecutor(), cs.MessageBus())

    @resource(name="connector")
    def connector_provider():
        yield connector
    engine.register(connector_provider)

    agent_tasks = []
    for i in range(NUM_AGENTS):
        initial_phase = random.uniform(0, PERIOD)
        neighbor_ids = get_neighbors(i, GRID_SIDE, GRID_SIDE)
        my_neighbors = [channels[nid] for nid in neighbor_ids]
        my_channel = channels[i]

        workflow = firefly_agent(
            agent_id=i,
            initial_phase=initial_phase,
            period=PERIOD,
            nudge=NUDGE,
            neighbors=my_neighbors,
            my_channel=my_channel,
            connector=cs.inject("connector"),
            refractory_period=PERIOD * 0.2,
        )
        agent_tasks.append(engine.run(workflow))

    all_tasks = asyncio.gather(*agent_tasks)
    start_time = time.time()
    last_report_time = start_time

    try:
        while time.time() - start_time < DURATION_SECONDS:
            await asyncio.sleep(1.0)
            now = time.time()
            interval = now - last_report_time
            rate = (flash_count - (flash_times[0] if flash_times else 0)) / interval
            
            # Simple reporting
            current_rate = (flash_count - getattr(run_headless_experiment, 'last_flash_count', 0)) / interval
            print(f"   - Flashes/sec: {current_rate:,.0f} (Total: {flash_count:,})")
            
            run_headless_experiment.last_flash_count = flash_count
            last_report_time = now

    finally:
        print("\nCleaning up...")
        all_tasks.cancel()
        await asyncio.gather(all_tasks, return_exceptions=True)
        print("Test finished.")

# Add a stateful attribute for rate calculation
run_headless_experiment.last_flash_count = 0

if __name__ == "__main__":
    asyncio.run(run_headless_experiment())
~~~~~

##### è„šæœ¬ 3: æ€§èƒ½å‰–æžå…¥å£

è¿™ä¸ªè„šæœ¬æ˜¯ä¸€ä¸ªç®€åŒ–çš„ã€æ— å‚æ•°çš„å…¥å£ï¼Œæ–¹ä¾¿ `cProfile` è°ƒç”¨ã€‚

~~~~~act
write_file
observatory/scripts/profile_entry.py
~~~~~
~~~~~python
"""
Debug Script 3: Profiling Entry Point

Purpose:
  Provides a clean, simple entry point for running the full experiment
  with standard profiling tools like cProfile.

How to Run with cProfile:
  python -m cProfile -s tottime -o profile.out observatory/scripts/profile_entry.py
  
  Then, to view the stats:
  python -c "import pstats; p = pstats.Stats('profile.out'); p.sort_stats('tottime').print_stats(20)"

Expected Output:
  The full experiment will run for a short duration and then exit. The key
  output is the `profile.out` file, which contains detailed performance data.
  We will analyze this file to see which functions are consuming the most CPU time.
"""
import asyncio
from observatory.experiments import run_fireflies

# Use a smaller number of agents and shorter duration for quicker profiling runs
# And disable visualization as it adds noise to CPU profiles.
async def main():
    print("ðŸš€ Starting Profiling Run (15 seconds, headless)...")
    await run_fireflies.run_experiment(
        num_agents=900,  # 30x30 grid, a significant but manageable number
        duration_seconds=15.0,
        visualize=False
    )
    print("\nâœ… Profiling run finished.")

if __name__ == "__main__":
    asyncio.run(main())
~~~~~

### ä¸‹ä¸€æ­¥å»ºè®®

è¯·æŒ‰é¡ºåºæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼Œå¹¶å°†æ¯ä¸ªè„šæœ¬çš„**æœ€ç»ˆè¾“å‡º**æˆ–**æ¦‚è¦**å‘ç»™æˆ‘ï¼š

1.  **è¿è¡Œæ¸²æŸ“å™¨æµ‹è¯•**:
    ```bash
    python -m observatory.scripts.debug_renderer_throughput
    ```
    *(è®©å®ƒè¿è¡Œ30ç§’ï¼Œç„¶åŽæŒ‰ Ctrl+C åœæ­¢ã€‚è¯·åˆ†äº«ç”Ÿæˆçš„ `renderer_throughput_log_...jsonl` æ–‡ä»¶ä¸­çš„æœ€åŽå‡ è¡Œæ—¥å¿—ã€‚)*

2.  **è¿è¡Œæ— å¤´æ¨¡å¼æµ‹è¯•**:
    ```bash
    python -m observatory.scripts.debug_headless_throughput
    ```
    *(åŒæ ·ï¼Œè¿è¡Œ30ç§’åŽåœæ­¢ï¼Œå¹¶åˆ†äº«æœ€åŽå‡ è¡Œçš„ `Flashes/sec` è¾“å‡ºã€‚)*

3.  **è¿è¡Œæ€§èƒ½å‰–æž**:
    ```bash
    python -m cProfile -s tottime -o profile.out observatory/scripts/profile_entry.py
    ```
    ç„¶åŽï¼Œè¿è¡Œè¿™ä¸ªå‘½ä»¤æ¥æŸ¥çœ‹ç»“æžœï¼š
    ```bash
    python -c "import pstats; p = pstats.Stats('profile.out'); p.sort_stats('tottime').print_stats(30)"
    ```
    *(è¯·å°†è¿™ä¸ªå‘½ä»¤çš„**å®Œæ•´è¾“å‡º**å‘ç»™æˆ‘ã€‚)*

æœ‰äº†è¿™ä¸‰ä»½æ•°æ®ï¼Œæˆ‘ä»¬å°±èƒ½åƒåšCTæ‰«æä¸€æ ·ï¼Œç²¾ç¡®åœ°å®šä½åˆ°æ€§èƒ½è‚¿ç˜¤ã€‚
